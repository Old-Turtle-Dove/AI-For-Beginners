{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56546290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name Dataset size: 15\n",
      "Feature Content Dataset size: 382688\n",
      "First Feature Name Sample: {'feature_id': 101, 'feature_name': 'news_culture'}\n",
      "First Feature Content Sample: {'feature_id': 101, 'content': '京城最值得你来场文化之旅的博物馆'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FeatureNameDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path, sep='_!_', header=None, engine='python', names=['id','feature_id', 'feature_name', 'content'])\n",
    "        self.data = self.data.drop_duplicates(subset=['feature_id'])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_id = self.data.iloc[idx, 0]\n",
    "        feature_name = self.data.iloc[idx, 1]\n",
    "\n",
    "        sample = {'feature_id': feature_id, 'feature_name': feature_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class FeatureContentDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path, sep='_!_', header=None, engine='python', names=['id','feature_id', 'feature_name', 'content'])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_id = self.data.iloc[idx, 0]\n",
    "        content = self.data.iloc[idx, 2]\n",
    "\n",
    "        sample = {'feature_id': feature_id, 'content': content}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# 示例使用示范\n",
    "# 用\"_!_\"作为分隔符读取CSV文件\n",
    "file_path = '/home/liyi/gpt/data/toutiao_cat_data.txt'  # 请替换为您的文件路径\n",
    "feature_name_dataset = FeatureNameDataset(file_path)\n",
    "\n",
    "# 创建特征和内容对应关系的数据集\n",
    "feature_content_dataset = FeatureContentDataset(file_path)\n",
    "\n",
    "# 获取特征名称对应关系数据集的大小\n",
    "print(\"Feature Name Dataset size:\", len(feature_name_dataset))\n",
    "\n",
    "# 获取特征和内容对应关系数据集的大小\n",
    "print(\"Feature Content Dataset size:\", len(feature_content_dataset))\n",
    "\n",
    "# 获取第一个特征名称对应关系数据集的样本\n",
    "first_feature_name_sample = feature_name_dataset[0]\n",
    "print(\"First Feature Name Sample:\", first_feature_name_sample)\n",
    "\n",
    "# 获取第一个特征和内容对应关系数据集的样本\n",
    "first_feature_content_sample = feature_content_dataset[0]\n",
    "print(\"First Feature Content Sample:\", first_feature_content_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a72cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 306150\n",
      "Test Dataset size: 76538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(feature_content_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# 获取训练集和测试集的大小\n",
    "print(\"Train Dataset size:\", len(train_dataset))\n",
    "print(\"Test Dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab099f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def tokenize_chinese(text):\n",
    "    return list(jieba.cut(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43322c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.306 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47033/568111320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'京城最值得你来场文化之旅的博物馆'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ai4beg/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai4beg/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         \"\"\"\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1249\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \"string object received.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_chinese)\n",
    "contents = [item['content'] for item in train_dataset]\n",
    "vectorizer.fit_transform(contents)\n",
    "vectorizer.transform('京城最值得你来场文化之旅的博物馆')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c37e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 107134)\t0.09505972920299476\n",
      "  (0, 87647)\t0.5114845895888899\n",
      "  (0, 84660)\t0.208804151466844\n",
      "  (0, 79887)\t0.30393638164872777\n",
      "  (0, 39380)\t0.37424822892911064\n",
      "  (0, 27859)\t0.2765131215118262\n",
      "  (0, 26506)\t0.15514004706645676\n",
      "  (0, 22345)\t0.4650000889643618\n",
      "  (0, 19370)\t0.3695347136224028\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(['京城最值得你来场文化之旅的博物馆']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59be367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    contents = [item['content'] for item in b] \n",
    "    return (\n",
    "            torch.LongTensor([t['feature_id']-100 for t in b]),\n",
    "            torch.tensor(vectorizer.transform(contents).toarray(), dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "input_len = len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "892170de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['大学宿舍里最怕碰上这种人，简直无法容忍', '伊朗议员议会时烧毁美国国旗“处死美国”', '关西机场落地，如何快速方便的直接去京都？', '农村好多县城买房，什么原因？', '上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？', '请吃饭的孙艺珍姐姐，又在电影里狠狠虐了我一把！', '十五万块钱能入手的六款汽车，众泰SR9在列，你为哪一款疯狂？', '这豪车当初提车加价20万，国产后降10万却遭唾弃，奥迪宝马都笑了', '中国海军所有舰艇在海上遇到它都得鸣笛致敬，航母也不例外！', '烟台的特产有什么，有去过养马岛的吗？', '定了！红河县奕车“姑娘节”就在下周三！浪漫之旅穿越千年不老时光，你约吗？', '鲁HSM757、皖J65649、浙B0C809……敢在高速上做这种事？坚决严查！曝光！', '汇易资讯：国内部分油厂豆粕库存与未执行合同统计（18年第18周）', '为什么说鲁迅曾用薛宝钗影射高士奇？', '这才是亲情！她九岁摔倒致瘫，哥哥和弟弟不离不弃照顾近半个世纪']\n"
     ]
    }
   ],
   "source": [
    "first_15_samples = [train_dataset[i] for i in range(15)]\n",
    "contents = [item['content'] for item in first_15_samples] \n",
    "print(contents)\n",
    "result = vectorizer.transform(contents)\n",
    "dense_matrix = result.toarray()\n",
    "tensor = torch.tensor(dense_matrix, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ca3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(input_len,17),torch.nn.LogSoftmax(dim=1))\n",
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "264bcf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.7975\n",
      "6400: acc=0.8015625\n",
      "9600: acc=0.808125\n",
      "12800: acc=0.80953125\n",
      "16000: acc=0.810875\n",
      "19200: acc=0.8115104166666667\n",
      "22400: acc=0.8127678571428572\n",
      "25600: acc=0.8148046875\n",
      "28800: acc=0.8161111111111111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04906961811122609, 0.8168976545842217)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fb994ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.043203184075026456, Test Accuracy: 0.820468264130236\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "def test_model(net, dataloader):\n",
    "    net.eval()  # 设置模型为评估模式\n",
    "    total_loss, acc, count = 0, 0, 0\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for labels, features in dataloader:\n",
    "            out = net(features)\n",
    "            loss = loss_fn(out, labels)\n",
    "            total_loss += loss\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            acc += (predicted == labels).sum()\n",
    "            count += len(labels)\n",
    "\n",
    "    avg_loss = total_loss.item() / count\n",
    "    accuracy = acc.item() / count\n",
    "    print(f\"Test Loss: {avg_loss}, Test Accuracy: {accuracy}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "test_loss, test_accuracy = test_model(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56847bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8])\n",
      "{'feature_id': 108, 'feature_name': 'news_edu'}\n"
     ]
    }
   ],
   "source": [
    "input_tensor = vectorizer.transform(['没有爱，就没有教育！江门开平一幼儿园收到家长亲笔写的“情书”_!_美丽中国,幼儿园,牛津,江门,Nini,感谢信'])\n",
    "dense_matrix = input_tensor.toarray()\n",
    "tensor_2d = torch.tensor(dense_matrix, dtype=torch.float32)\n",
    "\n",
    "def getFeatureName(feature_id):\n",
    "    return next((item for item in feature_name_dataset if item['feature_id'] == feature_id), None)\n",
    "        \n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    out = net(tensor_2d)\n",
    "    probabilities = torch.exp(out)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    print(predicted_class)\n",
    "    print(getFeatureName(100+predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "244695eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(text):\n",
    "    input_tensor = vectorizer.transform([text])\n",
    "    dense_matrix = input_tensor.toarray()\n",
    "    tensor_2d = torch.tensor(dense_matrix, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        out = net(tensor_2d)\n",
    "        probabilities = torch.exp(out)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1)\n",
    "        print(predicted_class)\n",
    "        print(getFeatureName(100+predicted_class))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b8cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "{'feature_id': 102, 'feature_name': 'news_entertainment'}\n"
     ]
    }
   ],
   "source": [
    "get_class('佟丽娅和陈思成离婚了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "360409fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "{'feature_id': 102, 'feature_name': 'news_entertainment'}\n"
     ]
    }
   ],
   "source": [
    "get_class('佟丽娅又结婚了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8225526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "{'feature_id': 103, 'feature_name': 'news_sports'}\n"
     ]
    }
   ],
   "source": [
    "get_class('梅西又获得了金球奖')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f58e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "{'feature_id': 103, 'feature_name': 'news_sports'}\n"
     ]
    }
   ],
   "source": [
    "get_class('2024年的男篮世界杯开始')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e4a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
